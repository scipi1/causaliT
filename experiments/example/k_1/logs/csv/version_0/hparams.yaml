version: proT_5.5
experiment:
  d_model_set: 50
  e_layers: 1
  d_layers: 1
  n_heads: 1
  d_ff: 50
  d_qk: 20
  dropout: 0
model:
  model_object: proT
  embed_dim:
    d_model_set: 50
    d_base_emb: null
    d_base_model: null
    enc_pro_emb_hidden: 50
    enc_occ_emb_hidden: 50
    enc_step_emb_hidden: 50
    enc_pos_emb_hidden: 50
    enc_val_emb_hidden: 50
    enc_var_emb_hidden: 50
    enc_time_emb_hidden: 50
    dec_val_emb_hidden: 50
    dec_var_emb_hidden: 50
    dec_pos_emb_hidden: 50
    dec_time_emb_hidden: 50
  kwargs:
    model: proT
    comps_embed_enc: summation
    comps_embed_dec: summation
    ds_embed_enc:
      setting:
        sparse_grad: false
      modules:
      - idx: 1
        embed: nn_embedding
        label: variable
        kwargs:
          num_embeddings: 20
          embedding_dim: 50
          padding_idx: 0
          sparse: false
          max_norm: 1
      - idx: 0
        embed: linear
        label: value
        kwargs:
          input_dim: 1
          embedding_dim: 50
      - idx: 0
        embed: mask
        label: value_missing
        kwargs: {}
    ds_embed_dec:
      setting:
        sparse_grad: false
      modules:
      - idx: 1
        embed: nn_embedding
        label: variable
        kwargs:
          num_embeddings: 2
          embedding_dim: 50
      - idx: 0
        embed: linear
        label: value
        kwargs:
          input_dim: 1
          embedding_dim: 50
      - idx: 0
        embed: mask
        label: value_missing
        kwargs: {}
    enc_attention_type: ScaledDotProduct
    dec_self_attention_type: ScaledDotProduct
    dec_cross_attention_type: ScaledDotProduct
    enc_mask_type: Uniform
    dec_self_mask_type: Uniform
    dec_cross_mask_type: Uniform
    n_heads: 1
    enc_causal_mask: false
    dec_causal_mask: false
    e_layers: 1
    d_layers: 1
    activation: gelu
    norm: layer
    use_final_norm: true
    device: cuda
    out_dim: 1
    d_ff: 50
    d_model_enc: 50
    d_model_dec: 50
    d_qk: 20
    dropout_emb: 0
    dropout_data: 0
    dropout_attn_out: 0
    dropout_ff: 0
    enc_dropout_qkv: 0
    enc_attention_dropout: 0
    dec_self_dropout_qkv: 0
    dec_self_attention_dropout: 0
    dec_cross_dropout_qkv: 0
    dec_cross_attention_dropout: 0
training:
  optimizer: adamw
  lr: 0.0005
  batch_size: 50
  max_epochs: 100
  loss_fn: mse
  k_fold: 5
  seed: 42
  save_ckpt_every_n_epochs: 1000
  entropy_regularizer: false
  gamma: 0
data:
  dataset: example
  filename_input: ds.npz
  filename_target: ds.npz
  val_idx: 0
  test_ds_ixd: null
  max_data_size: null
  features:
    X:
      value: 0
      variable: 1
    'Y':
      value: 0
      variable: 1
special:
  mode: []
