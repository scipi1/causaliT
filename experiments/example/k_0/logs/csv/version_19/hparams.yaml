version: proT_5.5
model:
  model_object: proT
  embed_dim:
    d_model_set: null
    d_base_emb: null
    d_base_model: null
    enc_val_emb_hidden: 50
    enc_var_emb_hidden: 50
    enc_pos_emb_hidden: 70
    enc_time_emb_hidden: 90
    dec_val_emb_hidden: 130
    dec_var_emb_hidden: 100
    dec_pos_emb_hidden: 170
    dec_val_given_emb_hidden: 150
    dec_time_emb_hidden: 50
  kwargs:
    model: proT
    comps_embed_enc: concat
    comps_embed_dec: concat
    ds_embed_enc:
      setting:
        sparse_grad: false
      modules:
      - idx: 1
        embed: nn_embedding
        label: variable
        kwargs:
          num_embeddings: 404
          embedding_dim: 50
          padding_idx: 0
          sparse: false
          max_norm: 1
      - idx: 0
        embed: linear
        label: value
        kwargs:
          input_dim: 1
          embedding_dim: 50
      - idx: 0
        embed: mask
        label: value_missing
        kwargs: {}
    ds_embed_dec:
      setting:
        sparse_grad: false
      modules:
      - idx: 1
        embed: nn_embedding
        label: variable
        kwargs:
          num_embeddings: 3
          embedding_dim: 100
      - idx: 0
        embed: linear
        label: value
        kwargs:
          input_dim: 1
          embedding_dim: 130
      - idx: 0
        embed: mask
        label: value_missing
        kwargs: {}
    enc_attention_type: ScaledDotProduct
    dec_self_attention_type: ScaledDotProduct
    dec_cross_attention_type: ScaledDotProduct
    enc_mask_type: Uniform
    dec_self_mask_type: Uniform
    dec_cross_mask_type: Uniform
    n_heads: 2
    causal_mask: false
    enc_causal_mask: false
    dec_causal_mask: false
    e_layers: 1
    d_layers: 1
    activation: gelu
    norm: layer
    use_final_norm: true
    device: cuda
    out_dim: 2
    d_ff: 400
    d_model_enc: 100
    d_model_dec: 230
    d_qk: 100
    dropout_emb: 0.2
    dropout_data: 0.2
    dropout_attn_out: 0.0
    dropout_ff: 0.0
    enc_dropout_qkv: 0.1
    enc_attention_dropout: 0.0
    dec_self_dropout_qkv: 0.1
    dec_self_attention_dropout: 0.0
    dec_cross_dropout_qkv: 0.0
    dec_cross_attention_dropout: 0.0
training:
  optimization: 1
  base_lr: 0.001
  emb_lr: 0.01
  emb_start_lr: 0.1
  warmup_epochs: 1000
  switch_epoch: 500
  switch_step: 2000
  warmup_steps: 3000
  batch_size: 50
  max_epochs: 1000
  loss_fn: mse
  k_fold: 5
  seed: 42
  save_ckpt_every_n_epochs: 1000
  entropy_regularizer: true
  gamma: 0.05
  lam: 1.0
data:
  dataset: example
  filename_input: ds.npz
  filename_target: ds.npz
  val_idx: 0
  pos_idx: null
  test_ds_ixd: null
  max_data_size: null
  features:
    X:
      value: 0
      variable: 1
    'Y':
      value: 0
      variable: 1
special:
  mode: []
